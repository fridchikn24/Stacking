# -*- coding: utf-8 -*-
"""Sameeullah_File1_HW7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iaH3xZT8aOdUxNQoCTqWHOyqyRvmgP10
"""

# Commented out IPython magic to ensure Python compatibility.
from math import sqrt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats

pd.pandas.set_option('display.max_columns', None)
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

#test dataset

df_test=pd.read_csv('/content/drive/MyDrive/test.csv')
test = df_test.copy()

df=pd.read_csv('/content/drive/MyDrive/train.csv')

data=df.copy()

categorical = [var for var in data.columns if data[var].dtypes=='O']
print(categorical)
print(len(categorical))

numerical = [var for var in data.columns if data[var].dtypes!='O']
print(numerical)
print(len(numerical))

discrete = [var for var in numerical if len(data[var].unique()) < 20]
print(discrete)
print(f'There are {len(discrete)} discrete variables')

missing_data_columns= list(data.columns[data.isnull().mean()>0.0])
print(missing_data_columns)
len(missing_data_columns)

#divide dataset into train, test

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop(['Target','Id'], axis=1),
                                                    data['Target'],
                                                    test_size=0.2,
                                                    random_state=0)

X_train.shape, X_test.shape
X_train.head()
y_train.head()

df_test=df_test.drop(['Id'], axis=1)
df_test.shape

!pip install feature-engine

from sklearn.preprocessing import RobustScaler
from feature_engine.imputation import MeanMedianImputer
from imblearn.pipeline import Pipeline
from feature_engine.transformation import YeoJohnsonTransformer

data_pre_process = Pipeline([
    
    # missing data imputation 
   ('mean_median_imputation', MeanMedianImputer(imputation_method='median',
                                   variables=missing_data_columns)),
   
    # Transforming Numerical Variables
   ('yjt', YeoJohnsonTransformer()),

    # feature Scaling
     ('scaler', RobustScaler())
    
])

data_pre_process.fit(X_train,y_train)

X_train=pd.DataFrame(data_pre_process.transform(X_train),columns=X_train.columns)
X_test=pd.DataFrame(data_pre_process.transform(X_test),columns=X_test.columns)

df_test=pd.DataFrame(data_pre_process.transform(df_test),columns=df_test.columns)

#Making f2 scorer for Grid search CV
from sklearn.metrics import fbeta_score, make_scorer
ftwo_scorer = make_scorer(fbeta_score, beta=2)
ftwo_scorer

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from  sklearn.ensemble import ExtraTreesClassifier
from  sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import f1_score
from numpy import mean
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import fbeta_score
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import SVMSMOTE
from imblearn.over_sampling import ADASYN

import pickle

#Tried all possibilites for k neighbours like 10,50,100 to avoid overfitting but no luck

pipe_xgb_svmsmote = Pipeline([('svmsmote', SVMSMOTE()), ('model', XGBClassifier(random_state=42,early_stopping_rounds=2,tree_method = 'hist',
                                                                                objective= 'binary:logistic'))])
param_grid = {
    # try different feature engineering parameters
    'svmsmote__k_neighbors': [2],
    'model__n_estimators': [60],
    'model__max_depth': [5],
    'model__subsample': [0.9]
}

#apply grid search
grid_svmsmote_xgb= GridSearchCV(pipe_xgb_svmsmote, param_grid, cv=5, n_jobs=-1, scoring=ftwo_scorer)
grid_svmsmote_xgb.fit(X_train, y_train)
grid_svmsmote_xgb.best_estimator_

logreg = LogisticRegression()
param_grid = {
    
    'class_weight': [{0:1,1:10}],
    'C': [0.01], 
    'penalty': ['l2']
}
#apply grid search
grid_logreg= GridSearchCV(logreg, param_grid, cv=5, n_jobs=-1, scoring=ftwo_scorer)
grid_logreg.fit(X_train, y_train)

# GridSearch with oversampling
pipe_knn_adasyn = Pipeline([('adasyn', SVMSMOTE()), ('model', KNeighborsClassifier())])
param_grid = {
    # try different feature engineering parameters
    'adasyn__k_neighbors': [10],
    #'model__p': [3,4,5,6], 
   'model__n_neighbors' : [10], 
   #'model__weights': ['uniform','distance']
}

#apply grid search
grid_adasyn_knn= GridSearchCV(pipe_knn_adasyn, param_grid, cv=5, n_jobs=-1, scoring=ftwo_scorer)
grid_adasyn_knn.fit(X_train, y_train)

#class weight balanced, balanced_subsample
etc2 = ExtraTreesClassifier(random_state=42)
param_grid = {
    
    'class_weight': ['balanced'],
    'n_estimators': [500],
    'max_features': ['auto'],
    'max_depth' : [10],
    'criterion' :['gini']
}

#apply grid search
grid_etc2= GridSearchCV(etc2, param_grid, cv=5, n_jobs=-1, scoring=ftwo_scorer)
grid_etc2.fit(X_train, y_train)

from sklearn.neural_network import MLPClassifier 
pipe_mlp_svmsmote = Pipeline([('svmsmote', SVMSMOTE()), ('model', MLPClassifier())])
param_grid = {
    # try different feature engineering parameters
    'svmsmote__k_neighbors': [10],
    'model__alpha': [0.9],
    'model__solver': ['sgd']
}

#apply grid search
grid_svmsmote_mlp= GridSearchCV(pipe_mlp_svmsmote, param_grid, cv=5, n_jobs=-1, scoring=ftwo_scorer)
grid_svmsmote_mlp.fit(X_train, y_train)

print("Best parameters: {}".format(grid_svmsmote_mlp.best_params_))
print("Best Mean cross-validation score: {:.5f}".format(grid_svmsmote_mlp.best_score_))
print(f'Train score is {grid_svmsmote_mlp.score(X_train,y_train)}')
print(f'Test score is {grid_svmsmote_mlp.score(X_test,y_test)}')

stack1 =StackingClassifier(estimators = 
                            [('xgb_over', grid_svmsmote_xgb.best_estimator_),
                            ('knn_over', grid_adasyn_knn.best_estimator_),
                            ('cost_trees', grid_etc2.best_estimator_)
                            ], final_estimator = XGBClassifier(random_state=42,early_stopping_rounds=2,tree_method = 'hist',objective= 'binary:logistic'))

stack1_params = {
    'final_estimator__n_estimators':[50],
    'final_estimator__max_depth': [5],
    'final_estimator__subsample':[.9],
}

stack1_grid = GridSearchCV(stack1,stack1_params, cv = 5, n_jobs=-1,return_train_score=True)
stack1_grid.fit(X_train,y_train)

print(f'Best Mean Cross Validation Score is {stack1_grid.best_score_}')
print(f'Best Mean Cross Validation Score is {stack1_grid.best_params_}')
print(f'Train score is {stack1_grid.score(X_train,y_train)}')
print(f'Test score is {stack1_grid.score(X_test,y_test)}')

stack2 =StackingClassifier(estimators = 
                            [('xgb_over', grid_svmsmote_xgb.best_estimator_),
                            ('logreg', grid_logreg.best_estimator_),
                            ('mlp', grid_svmsmote_mlp.best_estimator_)
                            ], final_estimator = XGBClassifier(random_state=42,tree_method = 'hist',early_stopping_rounds=2,objective= 'binary:logistic'))

stack2_params = {
    'final_estimator__n_estimators':[60],
    'final_estimator__max_depth': [5],
    'final_estimator__subsample':[1],
}

stack2_grid = GridSearchCV(stack2,stack2_params, cv = 5, n_jobs=-1,return_train_score=True)
stack2_grid.fit(X_train,y_train)

print(f'Best Mean Cross Validation Score is {stack2_grid.best_score_}')
print(f'Best Mean Cross Validation Score is {stack2_grid.best_params_}')
print(f'Train score is {stack2_grid.score(X_train,y_train)}')
print(f'Test score is {stack2_grid.score(X_test,y_test)}')

"""The first stacking method has a slightly lower test score but it takes significantly less time to train, with a 13 minute deficit between the first and second stacking method. This is likely due to the application of a nueral network in the 2nd stacking model's base.

"""

!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py
from colab_pdf import colab_pdf
colab_pdf('Sameeullah_File1_HW7.ipynb')